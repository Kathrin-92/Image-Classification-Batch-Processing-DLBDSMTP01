services:
  mlflow:
    build:
      context: ./model
      dockerfile: Dockerfile
    container_name: mlflow_container
    ports:
      - "8080:8080"
    environment:
      MLFLOW_TRACKING_URI: http://0.0.0.0:8080
      MLFLOW_ARTIFACT_ROOT: /usr/src/mlartifacts  # path to shared volume for artifacts
    volumes:
      - mlflow-artifacts:/usr/src/mlartifacts  # shared volume for artifacts
    networks:
      - app_network
    restart: always

  fastapi:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: fastapi_container
    ports:
      - "8000:8000"
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:8080  # access MLflow server from FastAPI
    volumes:
      - mlflow-artifacts:/usr/src/mlartifacts  # same volume for artifact access
    networks:
      - app_network
    depends_on:
      - mlflow
    healthcheck:
      test: curl --fail http://localhost:8000/health || exit 1
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: on-failure

  batch_process:
    build:
      context: ./batch_process
      dockerfile: Dockerfile
    container_name: batch_process_container
    volumes:
      - ./prediction-results:/api/prediction-results
      - ./upload:/api/upload
    networks:
      - app_network
    restart: on-failure
    depends_on:
      - fastapi
      - mlflow


volumes:
  mlflow-artifacts:
    driver: local
  upload:
    driver: local
  prediction-results:
    driver: local


networks:
  app_network:
    driver: bridge
